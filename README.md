Links Used:

Grimmâ€™s fairy tail generation: https://blog.goodaudience.com/writing-fairy-tales-using-ai-a64d0ecb1ddc 

Tensorflow text generation tutorial: 
https://www.tensorflow.org/tutorials/text/text_generation 
English Hw completion via NN: https://towardsdatascience.com/getting-a-machine-to-do-my-english-homework-for-me-5d339470fe42

Lovecraft generation: https://www.kdnuggets.com/2019/07/training-neural-network-write-like-lovecraft.html 

Word-based nursery rhyme model:
https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/

Word embedding types:
https://www.tensorflow.org/tutorials/text/word2vec
https://datascience.stackexchange.com/questions/13138/what-is-the-difference-between-word-based-and-char-based-text-generation-rnns

The math behind RNNs and LSTMs:
https://www.quora.com/How-do-I-understand-the-architecture-and-maths-of-an-LSTM-neural-network
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
GRU vs LSTM structure:
https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21

